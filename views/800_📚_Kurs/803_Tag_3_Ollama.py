"""
Tag 3: Ollama

Lokale KI-Modelle mit Ollama nutzen
"""

import streamlit as st
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from lib.helper_streamlit import show_code, add_select_model
from lib.helper_ollama import check_ollama_status, get_available_models

st.set_page_config(
    page_title="Tag 3: Ollama",
    page_icon="ü¶ô",
    layout="wide"
)

st.title("ü¶ô Tag 3: Ollama")
st.markdown("**Lokale KI-Modelle mit Ollama nutzen**")

# Tabs
TAB_OVERVIEW = "√úbersicht"
TAB_SETUP = "Setup"
TAB_MODELS = "Modelle"
TAB_API_BASICS = "API Basics"
TAB_CHAT = "Chat"
TAB_STREAMING = "Streaming"
TAB_INTEGRATION = "Integration"
TAB_EXERCISES = "√úbungen"

TAB_NAMES = [
    TAB_OVERVIEW,
    TAB_SETUP,
    TAB_MODELS,
    TAB_API_BASICS,
    TAB_CHAT,
    TAB_STREAMING,
    TAB_INTEGRATION,
    TAB_EXERCISES,
]

tabs = st.tabs(TAB_NAMES)

def get_tab_index(name):
    try:
        return TAB_NAMES.index(name)
    except ValueError:
        return -1

# Tab 1: √úbersicht
with tabs[get_tab_index(TAB_OVERVIEW)]:
    st.header("üìã Kurs√ºbersicht Tag 3")
    
    st.markdown("""
    ### Lernziele
    Am Ende von Tag 3 k√∂nnen Sie:
    - ‚úÖ Ollama installieren und konfigurieren
    - ‚úÖ KI-Modelle verwalten
    - ‚úÖ Ollama Python API nutzen
    - ‚úÖ Text generieren und chatten
    - ‚úÖ Streamlit mit Ollama verbinden
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### üìö Themen
        1. **Was ist Ollama?**
           - Lokale LLMs
           - Vorteile
           - Use Cases
        
        2. **Installation & Setup**
           - Ollama installieren
           - Server starten
           - Status pr√ºfen
        
        3. **Modell-Management**
           - Modelle installieren
           - Modelle auflisten
           - Modelle l√∂schen
        
        4. **Python API**
           - Generate
           - Chat
           - Streaming
           - Embeddings
        """)
    
    with col2:
        st.markdown("""
        ### ‚è±Ô∏è Zeitplan
        - **09:00 - 10:00**: Installation & Setup
        - **10:00 - 10:15**: Pause
        - **10:15 - 11:30**: Modell-Management
        - **11:30 - 12:00**: API Grundlagen
        - **12:00 - 13:00**: Mittagspause
        - **13:00 - 14:00**: Chat & Streaming
        - **14:00 - 15:00**: Streamlit Integration
        
        ### üîó Links
        - [ollama.ai](https://ollama.ai)
        - [Modelle](https://ollama.ai/library)
        - [GitHub](https://github.com/ollama/ollama)
        """)

# Tab 2: Setup
with tabs[get_tab_index(TAB_SETUP)]:
    st.header("1Ô∏è‚É£ Ollama Setup")
    
    st.markdown("""
    ### Was ist Ollama?
    Ollama erm√∂glicht es, **Large Language Models (LLMs)** lokal auf Ihrem Computer auszuf√ºhren.
    
    **Vorteile:**
    - üîí **Privatsph√§re**: Daten bleiben lokal
    - üí∞ **Kostenlos**: Keine API-Kosten
    - ‚ö° **Schnell**: Keine Netzwerk-Latenz
    - üîå **Offline**: Keine Internetverbindung n√∂tig
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### üì• Installation
        
        **macOS:**
        ```bash
        curl https://ollama.ai/install.sh | sh
        ```
        
        **Linux:**
        ```bash
        curl https://ollama.ai/install.sh | sh
        ```
        
        **Windows:**
        Laden Sie den Installer von [ollama.ai](https://ollama.ai) herunter.
        
        ### ‚úÖ Verifizierung
        ```bash
        ollama --version
        ```
        
        ### üöÄ Server starten
        ```bash
        ollama serve
        ```
        Der Server l√§uft auf `http://localhost:11434`
        """)
    
    with col2:
        st.markdown("""
        ### üêç Python SDK installieren
        ```bash
        pip install ollama
        ```
        
        ### üìù Erster Test
        ```python
        import ollama

        response = ollama.generate(
            model='llama3.2',
            prompt='Hello!'
        )
        print(response['response'])
        ```
        """)
        
        st.divider()
        
        st.markdown("### üîå Status pr√ºfen")
        
        if st.button("Ollama Status pr√ºfen", key="check_status"):
            with st.spinner("Pr√ºfe Ollama..."):
                status = check_ollama_status()
                
                if status['status'] == 'success':
                    st.success(f"‚úÖ {status['message']}")
                    st.metric("Installierte Modelle", len(status['models']))
                else:
                    st.error(f"‚ùå {status['message']}")
                    st.info("Stellen Sie sicher, dass Ollama installiert und gestartet ist.")

# Tab 3: Modelle
with tabs[get_tab_index(TAB_MODELS)]:
    st.header("2Ô∏è‚É£ Modell-Management")
    
    st.markdown("### üì¶ Beliebte Modelle")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **Allzweck-Modelle:**
        
        | Modell | Gr√∂√üe | Beschreibung |
        |--------|-------|--------------|
        | llama3.2 | 2GB | Schnell, gut f√ºr Chat |
        | llama3.1 | 4.7GB | Gr√∂√üer, pr√§ziser |
        | mistral | 4GB | Ausgewogen |
        | phi3 | 2GB | Leichtgewicht |
        
        **Spezialisierte Modelle:**
        
        | Modell | Gr√∂√üe | Spezialisierung |
        |--------|-------|-----------------|
        | codellama | 4GB | Programmierung |
        | llava | 4.7GB | Vision (Bilder) |
        | gemma2 | 5GB | Google Modell |
        """)
    
    with col2:
        st.markdown("""
        ### üì• Modell installieren
        ```bash
        ollama pull llama3.2
        ollama pull mistral
        ollama pull codellama
        ```
        
        ### üìã Modelle auflisten
        ```bash
        ollama list
        ```
        
        ### üóëÔ∏è Modell l√∂schen
        ```bash
        ollama rm llama3.2
        ```
        
        ### üß™ Modell testen
        ```bash
        ollama run llama3.2 "Hallo!"
        ```
        """)
    
    st.divider()
    
    st.markdown("### üìä Ihre Modelle")
    
    if st.button("Modelle neu laden", key="reload_models"):
        st.rerun()
    
    available_models = get_available_models()
    
    if available_models:
        st.success(f"‚úÖ {len(available_models)} Modelle gefunden")
        
        import pandas as pd
        models_list = []
        
        try:
            import ollama
            for model_name in available_models:
                try:
                    info = ollama.show(model_name)
                    size = info.get('size', 0)
                    models_list.append({
                        'Modell': model_name,
                        'Gr√∂√üe': f"{size / (1024**3):.2f} GB" if size > 0 else "N/A"
                    })
                except:
                    models_list.append({
                        'Modell': model_name,
                        'Gr√∂√üe': "N/A"
                    })
            
            if models_list:
                st.dataframe(pd.DataFrame(models_list), use_container_width=True)
        except Exception as e:
            st.error(f"Fehler beim Laden der Modelldetails: {e}")
            # Fallback: Nur Namen anzeigen
            for model in available_models:
                st.write(f"- {model}")
    else:
        st.warning("‚ö†Ô∏è Keine Modelle gefunden")
        st.info("Installieren Sie ein Modell mit `ollama pull llama3.2`")

# Tab 4: API Basics
with tabs[get_tab_index(TAB_API_BASICS)]:
    st.header("3Ô∏è‚É£ Ollama Python API")
    
    st.markdown("### üìù Generate - Text generieren")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.code('''import ollama

# Einfache Generierung
response = ollama.generate(
    model='llama3.2',
    prompt='Erkl√§re Python in einem Satz'
)

print(response['response'])

# Mit Optionen
response = ollama.generate(
    model='llama3.2',
    prompt='Schreibe ein Gedicht',
    options={
        'temperature': 0.9,  # Kreativit√§t
        'top_p': 0.9,
        'top_k': 40,
        'num_predict': 200   # Max Tokens
    }
)

print(response['response'])
''', language='python')
    
    with col2:
        st.markdown("**Live Test:**")
        
        available_models = get_available_models()
        
        if available_models:
            gen_model = st.selectbox("Modell:", available_models, key="gen_model")
            gen_prompt = st.text_area(
                "Prompt:",
                "Erkl√§re Ollama in einem Satz",
                height=100,
                key="gen_prompt"
            )
            
            gen_temp = st.slider("Temperature:", 0.0, 2.0, 0.7, 0.1, key="gen_temp")
            
            if st.button("Generieren", key="gen_button"):
                with st.spinner("Generiere..."):
                    try:
                        import ollama
                        response = ollama.generate(
                            model=gen_model,
                            prompt=gen_prompt,
                            options={'temperature': gen_temp}
                        )
                        st.success("‚úÖ Antwort:")
                        st.write(response['response'])
                    except Exception as e:
                        st.error(f"‚ùå Fehler: {e}")
        else:
            st.warning("Keine Modelle verf√ºgbar")
    
    st.divider()
    
    st.markdown("### ‚öôÔ∏è Parameter-Guide")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        **Temperature**
        - `0.0-0.3`: Faktisch, deterministisch
        - `0.4-0.7`: Ausgewogen (Standard)
        - `0.8-2.0`: Kreativ, variabel
        
        *F√ºr Code: 0.1-0.3*  
        *F√ºr Stories: 0.8-1.5*
        """)
    
    with col2:
        st.markdown("""
        **Top P** (Nucleus Sampling)
        - Werte: `0.0-1.0`
        - Standard: `0.9`
        - Steuert Token-Auswahl
        
        *H√∂her = mehr Vielfalt*
        """)
    
    with col3:
        st.markdown("""
        **Num Predict**
        - Max. generierte Tokens
        - Standard: variiert
        - Begrenzt Antwortl√§nge
        
        *100 = kurze Antwort*  
        *1000 = lange Antwort*
        """)

# Tab 5: Chat
with tabs[get_tab_index(TAB_CHAT)]:
    st.header("4Ô∏è‚É£ Chat API")
    
    st.markdown("### üí¨ Chat mit Konversations-History")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.code('''import ollama

# Chat mit History
messages = [
    {
        'role': 'system',
        'content': 'Du bist ein hilfreicher Assistent.'
    },
    {
        'role': 'user',
        'content': 'Hallo, wer bist du?'
    }
]

response = ollama.chat(
    model='llama3.2',
    messages=messages
)

# Antwort ausgeben
print(response['message']['content'])

# Antwort zur History hinzuf√ºgen
messages.append(response['message'])

# N√§chste Frage
messages.append({
    'role': 'user',
    'content': 'Was kannst du?'
})

response = ollama.chat(
    model='llama3.2',
    messages=messages
)
''', language='python')
    
    with col2:
        st.markdown("**Live Chat:**")
        
        available_models = get_available_models()
        
        if available_models:
            chat_model = st.selectbox("Modell:", available_models, key="chat_model")
            
            # System Prompt
            system_prompt = st.text_area(
                "System Prompt:",
                "Du bist ein hilfreicher Assistent.",
                height=80,
                key="system_prompt"
            )
            
            # Chat History
            if 'simple_chat' not in st.session_state:
                st.session_state.simple_chat = []
            
            # Display messages
            for msg in st.session_state.simple_chat:
                role_icon = "üë§" if msg["role"] == "user" else "ü§ñ"
                st.markdown(f"{role_icon} **{msg['role'].title()}:** {msg['content']}")
            
            # Input
            chat_input = st.text_input("Nachricht:", key="chat_input")
            
            col_send, col_clear = st.columns(2)
            
            with col_send:
                if st.button("Senden", key="send_btn"):
                    if chat_input:
                        st.session_state.simple_chat.append({
                            "role": "user",
                            "content": chat_input
                        })
                        
                        with st.spinner("Denke nach..."):
                            try:
                                import ollama
                                messages = [{"role": "system", "content": system_prompt}]
                                messages.extend(st.session_state.simple_chat)
                                
                                response = ollama.chat(
                                    model=chat_model,
                                    messages=messages
                                )
                                
                                st.session_state.simple_chat.append({
                                    "role": "assistant",
                                    "content": response['message']['content']
                                })
                                
                                st.rerun()
                            except Exception as e:
                                st.error(f"Fehler: {e}")
            
            with col_clear:
                if st.button("L√∂schen", key="clear_btn"):
                    st.session_state.simple_chat = []
                    st.rerun()
        else:
            st.warning("Keine Modelle verf√ºgbar")

# Tab 6: Streaming
with tabs[get_tab_index(TAB_STREAMING)]:
    st.header("5Ô∏è‚É£ Streaming")
    
    st.markdown("""
    ### ‚ö° Streaming f√ºr bessere UX
    Streaming zeigt die Antwort **w√§hrend der Generierung** - √§hnlich wie ChatGPT.
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.code('''import ollama

# Streaming mit generate
stream = ollama.generate(
    model='llama3.2',
    prompt='Erz√§hle eine Geschichte',
    stream=True
)

# Token f√ºr Token ausgeben
for chunk in stream:
    if 'response' in chunk:
        print(chunk['response'], end='', flush=True)

print()  # Neue Zeile am Ende

# Streaming mit chat
stream = ollama.chat(
    model='llama3.2',
    messages=[{
        'role': 'user',
        'content': 'Erkl√§re KI'
    }],
    stream=True
)

for chunk in stream:
    if 'message' in chunk:
        content = chunk['message']['content']
        print(content, end='', flush=True)
''', language='python')
        
        st.divider()
        
        st.markdown("### üé® Streamlit Integration")
        st.code('''# Streaming in Streamlit
response_placeholder = st.empty()
full_response = ""

stream = ollama.generate(
    model='llama3.2',
    prompt=prompt,
    stream=True
)

for chunk in stream:
    if 'response' in chunk:
        full_response += chunk['response']
        # Cursor-Animation
        response_placeholder.markdown(full_response + "‚ñå")

# Finales Ergebnis
response_placeholder.markdown(full_response)
''', language='python')
    
    with col2:
        st.markdown("**Live Streaming:**")
        
        available_models = get_available_models()
        
        if available_models:
            stream_model = st.selectbox("Modell:", available_models, key="stream_model")
            stream_prompt = st.text_input(
                "Prompt:",
                "Z√§hle von 1 bis 10",
                key="stream_prompt"
            )
            
            if st.button("Mit Streaming generieren", key="stream_btn"):
                response_placeholder = st.empty()
                full_response = ""
                
                try:
                    import ollama
                    stream = ollama.generate(
                        model=stream_model,
                        prompt=stream_prompt,
                        stream=True
                    )
                    
                    for chunk in stream:
                        if 'response' in chunk:
                            full_response += chunk['response']
                            response_placeholder.markdown(full_response + "‚ñå")
                    
                    response_placeholder.markdown(full_response)
                    
                except Exception as e:
                    st.error(f"Fehler: {e}")
        else:
            st.warning("Keine Modelle verf√ºgbar")

# Tab 7: Integration
with tabs[get_tab_index(TAB_INTEGRATION)]:
    st.header("6Ô∏è‚É£ Streamlit + Ollama Integration")
    
    st.markdown("### üéØ Vollst√§ndiges Chat-Beispiel")
    
    st.code('''import streamlit as st
import ollama

st.title("ü§ñ KI Chat-Assistent")

# Sidebar
with st.sidebar:
    st.header("Einstellungen")
    model = st.selectbox("Modell:", ["llama3.2", "mistral"])
    temperature = st.slider("Temperature:", 0.0, 2.0, 0.7, 0.1)

# Chat History
if 'messages' not in st.session_state:
    st.session_state.messages = []

# System Prompt
system_prompt = st.text_area(
    "System Prompt:",
    "Du bist ein hilfreicher Assistent."
)

# Nachrichten anzeigen
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Chat Input
if prompt := st.chat_input("Nachricht..."):
    # User message
    st.session_state.messages.append({
        "role": "user",
        "content": prompt
    })
    
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Assistant response mit Streaming
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(st.session_state.messages)
        
        stream = ollama.chat(
            model=model,
            messages=messages,
            stream=True,
            options={'temperature': temperature}
        )
        
        for chunk in stream:
            if 'message' in chunk:
                full_response += chunk['message']['content']
                message_placeholder.markdown(full_response + "‚ñå")
        
        message_placeholder.markdown(full_response)
        
        st.session_state.messages.append({
            "role": "assistant",
            "content": full_response
        })

# Clear Button
if st.button("üóëÔ∏è Chat l√∂schen"):
    st.session_state.messages = []
    st.rerun()
''', language='python')
    
    st.divider()
    
    st.markdown("### üí° Best Practices")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **Performance:**
        - ‚ö° Streaming f√ºr bessere UX
        - üéØ Niedrige Temperature f√ºr faktische Antworten
        - üß† Kleine Modelle (2-4GB) f√ºr schnelle Antworten
        - üì¶ Gro√üe Modelle (7GB+) f√ºr komplexe Aufgaben
        """)
    
    with col2:
        st.markdown("""
        **Fehlerbehandlung:**
        - ‚úÖ Ollama Status pr√ºfen
        - üîÑ Try-Except Bl√∂cke
        - üí¨ Benutzer-Feedback
        - üîå Connection Timeouts
        """)
    
    st.divider()
    
    st.markdown("### üéØ √úbungen")
    
    # Sub-Tabs f√ºr jede √úbung
    exercise_tabs = st.tabs([
        "üìÑ Zusammenfassung",
        "üåç √úbersetzer",
        "üíª Code-Erkl√§rer",
        "üìñ Story-Generator"
    ])
    
    # Text-Zusammenfassung
    with exercise_tabs[0]:
        st.markdown("""
        ### üìÑ Text-Zusammenfassung
        Erstellen Sie ein Tool zur automatischen Text-Zusammenfassung.
        
        **Anforderungen:**
        1. Text-Area f√ºr langen Text
        2. L√§ngen-Auswahl (kurz/mittel/lang)
        3. Ollama zur Zusammenfassung nutzen
        4. Wortanzahl vor/nach anzeigen
        """)
        st.markdown("""
        **Erstellen Sie ein Tool zur Text-Zusammenfassung:**
        1. Text-Area f√ºr langen Text
        2. L√§ngen-Auswahl (kurz/mittel/lang)
        3. Ollama zur Zusammenfassung nutzen
        4. Wortanzahl vor/nach anzeigen
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Ihre L√∂sung:**")
            
            available_models = get_available_models()
            
            if available_models:
                summary_text = st.text_area(
                    "Text zum Zusammenfassen:",
                    "Python ist eine interpretierte Hochsprache. Sie wurde von Guido van Rossum entwickelt. Python unterst√ºtzt mehrere Programmierparadigmen. Die Sprache ist sehr beliebt f√ºr Data Science und Machine Learning.",
                    height=150,
                    key="ex1_text"
                )
                
                summary_length = st.radio(
                    "L√§nge:",
                    ["Sehr kurz (1 Satz)", "Kurz (2-3 S√§tze)", "Mittel (1 Absatz)"],
                    key="ex1_length"
                )
                
                if st.button("Zusammenfassen", key="ex1_btn"):
                    with st.spinner("Fasse zusammen..."):
                        try:
                            import ollama
                            
                            prompt = f"Fasse diesen Text zusammen ({summary_length}): {summary_text}"
                            
                            response = ollama.generate(
                                model=available_models[0],
                                prompt=prompt,
                                options={'temperature': 0.3}
                            )
                            
                            st.success("‚úÖ Zusammenfassung:")
                            st.write(response['response'])
                            
                            col_a, col_b = st.columns(2)
                            col_a.metric("Original", f"{len(summary_text.split())} W√∂rter")
                            col_b.metric("Zusammenfassung", f"{len(response['response'].split())} W√∂rter")
                            
                        except Exception as e:
                            st.error(f"‚ùå Fehler: {e}")
            else:
                st.warning("Keine Modelle verf√ºgbar")
        
        with col2:
            st.markdown("**L√∂sungsansatz:**")
            st.code('''import streamlit as st
import ollama

text = st.text_area("Text:")
length = st.radio("L√§nge:", ["Kurz", "Mittel", "Lang"])

if st.button("Zusammenfassen"):
    prompt = f"Fasse zusammen ({length}): {text}"
    
    response = ollama.generate(
        model='llama3.2',
        prompt=prompt,
        options={'temperature': 0.3}
    )
    
    st.write(response['response'])
    
    # Statistiken
    original = len(text.split())
    summary = len(response['response'].split())
    st.metric("Reduktion", f"{(1 - summary/original)*100:.0f}%")
''', language='python')
    
    st.divider()
    
    st.markdown("#### Sprach-√úbersetzer")
    
    with st.expander("Aufgabe"):
        st.markdown("""
        **Erstellen Sie einen KI-√úbersetzer:**
        1. Text-Eingabe
        2. Zielsprache ausw√§hlen
        3. Mit Ollama √ºbersetzen
        4. √úbersetzung anzeigen
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Ihre L√∂sung:**")
            
            available_models = get_available_models()
            
            if available_models:
                translate_text = st.text_area(
                    "Text:",
                    "Hello, how are you today?",
                    key="ex2_text"
                )
                
                target_lang = st.selectbox(
                    "√úbersetzen nach:",
                    ["Deutsch", "Franz√∂sisch", "Spanisch", "Italienisch", "Japanisch"],
                    key="ex2_lang"
                )
                
                if st.button("√úbersetzen", key="ex2_btn"):
                    with st.spinner("√úbersetze..."):
                        try:
                            import ollama
                            
                            prompt = f"√úbersetze diesen Text nach {target_lang}: {translate_text}"
                            
                            response = ollama.generate(
                                model=available_models[0],
                                prompt=prompt,
                                options={'temperature': 0.3}
                            )
                            
                            st.success("‚úÖ √úbersetzung:")
                            st.write(response['response'])
                            
                        except Exception as e:
                            st.error(f"‚ùå Fehler: {e}")
            else:
                st.warning("Keine Modelle verf√ºgbar")
        
        with col2:
            st.markdown("**L√∂sungsansatz:**")
            st.code('''import streamlit as st
import ollama

text = st.text_area("Text zum √úbersetzen:")
target = st.selectbox(
    "Zielsprache:",
    ["Deutsch", "Englisch", "Franz√∂sisch"]
)

if st.button("√úbersetzen"):
    prompt = f"√úbersetze nach {target}: {text}"
    
    response = ollama.generate(
        model='llama3.2',
        prompt=prompt,
        options={'temperature': 0.3}
    )
    
    st.success("√úbersetzung:")
    st.write(response['response'])
''', language='python')
    
    # Code-Erkl√§rer
    with exercise_tabs[2]:
        st.markdown("""
        ### üíª Code-Erkl√§rer
        Erstellen Sie einen intelligenten Code-Erkl√§rer.
        
        **Anforderungen:**
        1. Code-Input (Text Area)
        2. Programmiersprache w√§hlen
        3. Ollama zur Erkl√§rung nutzen
        4. Erkl√§rung anzeigen
        """)
        st.markdown("""
        **Erstellen Sie einen Code-Erkl√§rer:**
        1. Code-Input (Text Area)
        2. Programmiersprache w√§hlen
        3. Ollama zur Erkl√§rung nutzen
        4. Erkl√§rung anzeigen
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Ihre L√∂sung:**")
            
            available_models = get_available_models()
            
            if available_models:
                code_input = st.text_area(
                    "Code:",
                    "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)",
                    height=150,
                    key="ex3_code"
                )
                
                lang = st.selectbox(
                    "Sprache:",
                    ["Python", "JavaScript", "Java", "C++"],
                    key="ex3_lang"
                )
                
                if st.button("Erkl√§ren", key="ex3_btn"):
                    with st.spinner("Analysiere Code..."):
                        try:
                            import ollama
                            
                            prompt = f"Erkl√§re diesen {lang} Code Zeile f√ºr Zeile:\n\n{code_input}"
                            
                            response = ollama.generate(
                                model=available_models[0],
                                prompt=prompt,
                                options={'temperature': 0.3}
                            )
                            
                            st.success("‚úÖ Erkl√§rung:")
                            st.write(response['response'])
                            
                        except Exception as e:
                            st.error(f"‚ùå Fehler: {e}")
            else:
                st.warning("Keine Modelle verf√ºgbar")
        
        with col2:
            st.markdown("**L√∂sungsansatz:**")
            st.code('''import streamlit as st
import ollama

code = st.text_area("Code:")
lang = st.selectbox("Sprache:", ["Python", "JavaScript"])

if st.button("Erkl√§ren"):
    prompt = f"Erkl√§re diesen {lang} Code:\\n\\n{code}"
    
    response = ollama.generate(
        model='llama3.2',
        prompt=prompt,
        options={'temperature': 0.3}
    )
    
    st.write(response['response'])
''', language='python')
    
    # Kreativ-Story-Generator
    with exercise_tabs[3]:
        st.markdown("""
        ### üìñ Kreativ-Story-Generator
        Erstellen Sie einen kreativen Story-Generator mit KI.
        
        **Anforderungen:**
        1. Genre ausw√§hlen
        2. Protagonist eingeben
        3. Setting ausw√§hlen
        4. Story-L√§nge w√§hlen
        5. Mit Ollama kreative Geschichte generieren
        6. Streaming f√ºr bessere UX
        """)
        st.markdown("""
        **Erstellen Sie einen kreativen Story-Generator:**
        1. Genre ausw√§hlen
        2. Protagonist eingeben
        3. Setting ausw√§hlen
        4. Story-L√§nge w√§hlen
        5. Mit Ollama kreative Geschichte generieren
        6. Streaming f√ºr bessere UX
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Ihre L√∂sung:**")
            
            available_models = get_available_models()
            
            if available_models:
                genre = st.selectbox(
                    "Genre:",
                    ["Science Fiction", "Fantasy", "Krimi", "Romance", "Abenteuer", "Horror"],
                    key="ex4_genre"
                )
                
                protagonist = st.text_input(
                    "Protagonist:",
                    "Alex",
                    key="ex4_protagonist"
                )
                
                setting = st.selectbox(
                    "Setting:",
                    ["Eine futuristische Stadt", "Ein magischer Wald", "Eine einsame Insel", 
                     "Eine Raumstation", "Ein altes Schloss"],
                    key="ex4_setting"
                )
                
                length = st.radio(
                    "Story-L√§nge:",
                    ["Kurz (100 W√∂rter)", "Mittel (300 W√∂rter)", "Lang (500 W√∂rter)"],
                    key="ex4_length"
                )
                
                if st.button("Story generieren", key="ex4_btn"):
                    with st.spinner("Schreibe Story..."):
                        try:
                            import ollama
                            
                            prompt = f"""Schreibe eine {length} {genre}-Geschichte √ºber {protagonist}.
                            
Setting: {setting}
                            
Die Geschichte soll spannend sein und einen √ºberraschenden Twist haben.
Schreibe kreativ und fesselnd!"""
                            
                            st.markdown("---")
                            st.markdown(f"**üìñ {genre}-Story: {protagonist}**")
                            
                            response_placeholder = st.empty()
                            full_story = ""
                            
                            stream = ollama.generate(
                                model=available_models[0],
                                prompt=prompt,
                                stream=True,
                                options={'temperature': 0.9}  # Hohe Kreativit√§t
                            )
                            
                            for chunk in stream:
                                if 'response' in chunk:
                                    full_story += chunk['response']
                                    response_placeholder.markdown(full_story + "‚ñå")
                            
                            response_placeholder.markdown(full_story)
                            
                            # Statistiken
                            word_count = len(full_story.split())
                            st.metric("W√∂rter", word_count)
                            
                            # Download Option
                            st.download_button(
                                "üì• Story herunterladen",
                                full_story,
                                f"story_{protagonist}_{genre}.txt",
                                "text/plain"
                            )
                            
                        except Exception as e:
                            st.error(f"‚ùå Fehler: {e}")
            else:
                st.warning("Keine Modelle verf√ºgbar")
        
        with col2:
            st.markdown("**L√∂sungsansatz:**")
            st.code('''import streamlit as st
import ollama

genre = st.selectbox("Genre:", ["Sci-Fi", "Fantasy"])
protagonist = st.text_input("Protagonist:", "Alex")
setting = st.selectbox("Setting:", ["Stadt", "Wald"])
length = st.radio("L√§nge:", ["Kurz", "Lang"])

if st.button("Story generieren"):
    prompt = f"""Schreibe eine {length} {genre}-Story 
√ºber {protagonist} in {setting}."""    
    
    placeholder = st.empty()
    full_story = ""
    
    # Streaming f√ºr bessere UX
    stream = ollama.generate(
        model='llama3.2',
        prompt=prompt,
        stream=True,
        options={'temperature': 0.9}  # Kreativ!
    )
    
    for chunk in stream:
        if 'response' in chunk:
            full_story += chunk['response']
            placeholder.markdown(full_story + "‚ñå")
    
    placeholder.markdown(full_story)
    
    # Download-Option
    st.download_button(
        "üì• Herunterladen",
        full_story,
        "story.txt"
    )
''', language='python')

# Tab 8: √úbungen
with tabs[get_tab_index(TAB_EXERCISES)]:
    st.header("7Ô∏è‚É£ Praktische √úbungen")
    
    st.markdown("""
    Hier sind 4 praktische √úbungen, um Ihr Ollama-Wissen zu festigen.
    Jede √úbung enth√§lt eine Aufgabenstellung und eine L√∂sung.
    """)
    
    # √úbungen in Sub-Tabs
    exercise_tabs = st.tabs([
        "Zusammenfassung",
        "√úbersetzer",
        "Code-Erkl√§rer",
        "Story-Generator"
    ])
    
    # Text-Zusammenfassung
    with exercise_tabs[0]:
        st.subheader("üìù Text-Zusammenfassung")
        
        st.markdown("""
        **Aufgabe:**
        Erstellen Sie ein Streamlit-Tool, das einen langen Text zusammenfasst.
        
        **Anforderungen:**
        1. Text-Eingabefeld f√ºr l√§ngeren Text
        2. Auswahl der Zusammenfassungsl√§nge (kurz/mittel/lang)
        3. Button zum Generieren der Zusammenfassung
        4. Anzeige der Zusammenfassung
        5. Bonus: Vergleich Original- vs. Zusammenfassungsl√§nge
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### üí° Live-Demo")
            
            # Model selector
            selected_model = add_select_model(key="practice_ex1_model")
            
            # Text input
            ex1_text = st.text_area(
                "Text zum Zusammenfassen:",
                """K√ºnstliche Intelligenz (KI) hat in den letzten Jahren enorme Fortschritte gemacht. 
Maschinelles Lernen, ein Teilbereich der KI, erm√∂glicht es Computern, aus Daten zu lernen und 
Vorhersagen zu treffen, ohne explizit programmiert zu werden. Deep Learning, eine spezielle Form 
des maschinellen Lernens, verwendet k√ºnstliche neuronale Netze mit vielen Schichten. Diese 
Technologie hat Durchbr√ºche in Bereichen wie Bilderkennung, Sprachverarbeitung und autonomes 
Fahren erm√∂glicht. Large Language Models wie GPT k√∂nnen nat√ºrliche Sprache verstehen und 
generieren. Trotz dieser Fortschritte gibt es auch Herausforderungen: Ethische Fragen, 
Datenschutzbedenken und die Notwendigkeit, KI-Systeme transparent und erkl√§rbar zu machen, 
sind wichtige Themen in der aktuellen Diskussion.""",
                height=200,
                key="practice_ex1_input"
            )
            
            # Length selection
            length = st.radio(
                "Zusammenfassungsl√§nge:",
                ["Kurz (1-2 S√§tze)", "Mittel (3-4 S√§tze)", "Lang (5+ S√§tze)"],
                key="practice_ex1_length"
            )
            
            if st.button("üìù Zusammenfassen", key="practice_ex1_button", type="primary"):
                if ex1_text:
                    length_map = {
                        "Kurz (1-2 S√§tze)": "in 1-2 S√§tzen",
                        "Mittel (3-4 S√§tze)": "in 3-4 S√§tzen",
                        "Lang (5+ S√§tze)": "in 5 oder mehr S√§tzen"
                    }
                    
                    prompt = f"Fasse den folgenden Text {length_map[length]} zusammen:\n\n{ex1_text}"
                    
                    with st.spinner("Zusammenfassung wird erstellt..."):
                        try:
                            import ollama
                            response = ollama.generate(
                                model=selected_model,
                                prompt=prompt,
                                options={'temperature': 0.3}
                            )
                            
                            st.success("‚úÖ Zusammenfassung erstellt!")
                            st.markdown("**Zusammenfassung:**")
                            st.write(response['response'])
                            
                            # Stats
                            with st.expander("üìä Statistiken"):
                                original_words = len(ex1_text.split())
                                summary_words = len(response['response'].split())
                                reduction = ((original_words - summary_words) / original_words) * 100
                                
                                col_a, col_b, col_c = st.columns(3)
                                col_a.metric("Original", f"{original_words} W√∂rter")
                                col_b.metric("Zusammenfassung", f"{summary_words} W√∂rter")
                                col_c.metric("Reduktion", f"{reduction:.0f}%")
                        except Exception as e:
                            st.error(f"Fehler: {str(e)}")
                else:
                    st.warning("Bitte geben Sie einen Text ein.")
        
        with col2:
            st.markdown("### üíª L√∂sung")
            st.code('''import streamlit as st
import ollama
from lib.helper_streamlit import add_select_model

st.title("Text-Zusammenfassung")

# Model selector
model = add_select_model()

# Text input
text = st.text_area("Text:", height=200)

# Length selection
length = st.radio(
    "L√§nge:",
    ["Kurz (1-2 S√§tze)", "Mittel (3-4 S√§tze)", "Lang (5+ S√§tze)"]
)

if st.button("Zusammenfassen"):
    length_map = {
        "Kurz (1-2 S√§tze)": "in 1-2 S√§tzen",
        "Mittel (3-4 S√§tze)": "in 3-4 S√§tzen",
        "Lang (5+ S√§tze)": "in 5 oder mehr S√§tzen"
    }
    
    prompt = f"Fasse den folgenden Text {length_map[length]} zusammen:\\n\\n{text}"
    
    response = ollama.generate(
        model=model,
        prompt=prompt,
        options={'temperature': 0.3}
    )
    
    st.write(response['response'])
    
    # Statistiken
    original_words = len(text.split())
    summary_words = len(response['response'].split())
    reduction = ((original_words - summary_words) / original_words) * 100
    
    st.metric("Reduktion", f"{reduction:.0f}%")
''', language='python')
    
    # √úbersetzer
    with exercise_tabs[1]:
        st.subheader("üåç Multi-Sprachen-√úbersetzer")
        
        st.markdown("""
        **Aufgabe:**
        Erstellen Sie einen √úbersetzer, der Text in verschiedene Sprachen √ºbersetzt.
        
        **Anforderungen:**
        1. Text-Eingabefeld
        2. Auswahl der Zielsprache (Englisch, Franz√∂sisch, Spanisch, Italienisch)
        3. Button zum √úbersetzen
        4. Anzeige der √úbersetzung
        5. Bonus: Streaming f√ºr l√§ngere Texte
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### üí° Live-Demo")
            
            selected_model = add_select_model(key="practice_ex2_model")
            
            ex2_text = st.text_area(
                "Text zum √úbersetzen:",
                "Hallo, wie geht es dir? Ich lerne gerade, wie man Ollama mit Streamlit verwendet.",
                height=100,
                key="practice_ex2_input"
            )
            
            target_lang = st.selectbox(
                "Zielsprache:",
                ["Englisch", "Franz√∂sisch", "Spanisch", "Italienisch", "Japanisch"],
                key="practice_ex2_lang"
            )
            
            use_streaming = st.checkbox("Streaming verwenden", value=True, key="practice_ex2_stream")
            
            if st.button("üåç √úbersetzen", key="practice_ex2_button", type="primary"):
                if ex2_text:
                    prompt = f"√úbersetze den folgenden Text ins {target_lang}e. Gib nur die √úbersetzung zur√ºck, ohne zus√§tzliche Erkl√§rungen:\n\n{ex2_text}"
                    
                    with st.spinner("√úbersetzung l√§uft..."):
                        try:
                            import ollama
                            
                            if use_streaming:
                                st.markdown("**√úbersetzung:**")
                                placeholder = st.empty()
                                translation = ""
                                
                                stream = ollama.generate(
                                    model=selected_model,
                                    prompt=prompt,
                                    stream=True
                                )
                                
                                for chunk in stream:
                                    if 'response' in chunk:
                                        translation += chunk['response']
                                        placeholder.markdown(translation + "‚ñå")
                                
                                placeholder.markdown(translation)
                            else:
                                response = ollama.generate(
                                    model=selected_model,
                                    prompt=prompt
                                )
                                st.success("‚úÖ √úbersetzung abgeschlossen!")
                                st.markdown("**√úbersetzung:**")
                                st.write(response['response'])
                        except Exception as e:
                            st.error(f"Fehler: {str(e)}")
                else:
                    st.warning("Bitte geben Sie einen Text ein.")
        
        with col2:
            st.markdown("### üíª L√∂sung")
            st.code('''import streamlit as st
import ollama
from lib.helper_streamlit import add_select_model

st.title("Multi-Sprachen-√úbersetzer")

model = add_select_model()

text = st.text_area("Text:", height=100)

target_lang = st.selectbox(
    "Zielsprache:",
    ["Englisch", "Franz√∂sisch", "Spanisch", "Italienisch"]
)

use_streaming = st.checkbox("Streaming", value=True)

if st.button("√úbersetzen"):
    prompt = f"√úbersetze den folgenden Text ins {target_lang}e:\\n\\n{text}"
    
    if use_streaming:
        placeholder = st.empty()
        translation = ""
        
        stream = ollama.generate(
            model=model,
            prompt=prompt,
            stream=True
        )
        
        for chunk in stream:
            if 'response' in chunk:
                translation += chunk['response']
                placeholder.markdown(translation + "‚ñå")
        
        placeholder.markdown(translation)
    else:
        response = ollama.generate(model=model, prompt=prompt)
        st.write(response['response'])
''', language='python')
    
    # Code-Erkl√§rer
    with exercise_tabs[2]:
        st.subheader("üíª Code-Erkl√§rer")
        
        st.markdown("""
        **Aufgabe:**
        Erstellen Sie ein Tool, das Code-Snippets analysiert und erkl√§rt.
        
        **Anforderungen:**
        1. Code-Eingabefeld (mit Syntax-Highlighting)
        2. Auswahl der Programmiersprache
        3. Erkl√§rungs-Level (Anf√§nger/Fortgeschritten/Experte)
        4. Button zum Analysieren
        5. Strukturierte Ausgabe der Erkl√§rung
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### üí° Live-Demo")
            
            selected_model = add_select_model(key="practice_ex3_model")
            
            prog_lang = st.selectbox(
                "Programmiersprache:",
                ["Python", "JavaScript", "Java", "C++", "SQL"],
                key="practice_ex3_lang"
            )
            
            ex3_code = st.text_area(
                "Code-Snippet:",
                '''def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

result = fibonacci(10)
print(f"Fibonacci(10) = {result}")''',
                height=150,
                key="practice_ex3_input"
            )
            
            level = st.radio(
                "Erkl√§rungs-Level:",
                ["Anf√§nger", "Fortgeschritten", "Experte"],
                key="practice_ex3_level"
            )
            
            if st.button("üí° Code erkl√§ren", key="practice_ex3_button", type="primary"):
                if ex3_code:
                    prompt = f"""Erkl√§re den folgenden {prog_lang}-Code f√ºr einen {level}. 
Strukturiere die Erkl√§rung wie folgt:
1. Was macht der Code? (Kurze Zusammenfassung)
2. Schritt-f√ºr-Schritt Erkl√§rung
3. Wichtige Konzepte
4. M√∂gliche Verbesserungen

Code:
{ex3_code}"""
                    
                    with st.spinner("Code wird analysiert..."):
                        try:
                            import ollama
                            response = ollama.generate(
                                model=selected_model,
                                prompt=prompt,
                                options={'temperature': 0.4}
                            )
                            
                            st.success("‚úÖ Analyse abgeschlossen!")
                            st.markdown("**Code-Erkl√§rung:**")
                            st.markdown(response['response'])
                        except Exception as e:
                            st.error(f"Fehler: {str(e)}")
                else:
                    st.warning("Bitte geben Sie ein Code-Snippet ein.")
        
        with col2:
            st.markdown("### üíª L√∂sung")
            st.code('''import streamlit as st
import ollama
from lib.helper_streamlit import add_select_model

st.title("Code-Erkl√§rer")

model = add_select_model()

prog_lang = st.selectbox(
    "Sprache:",
    ["Python", "JavaScript", "Java", "C++"]
)

code = st.text_area("Code:", height=150)

level = st.radio(
    "Erkl√§rungs-Level:",
    ["Anf√§nger", "Fortgeschritten", "Experte"]
)

if st.button("Code erkl√§ren"):
    prompt = f"""Erkl√§re den folgenden {prog_lang}-Code 
f√ºr einen {level}. Strukturiere die Erkl√§rung:
1. Was macht der Code?
2. Schritt-f√ºr-Schritt Erkl√§rung
3. Wichtige Konzepte
4. Verbesserungen

Code:
{code}"""
    
    response = ollama.generate(
        model=model,
        prompt=prompt,
        options={'temperature': 0.4}
    )
    
    st.markdown(response['response'])
''', language='python')
    
    # Story-Generator
    with exercise_tabs[3]:
        st.subheader("üìö Kreativer Story-Generator")
        
        st.markdown("""
        **Aufgabe:**
        Erstellen Sie einen kreativen Story-Generator mit Streaming.
        
        **Anforderungen:**
        1. Eingabefelder f√ºr Genre, Hauptcharakter, Setting
        2. Slider f√ºr Story-L√§nge
        3. Temperature-Slider f√ºr Kreativit√§t
        4. Streaming-Ausgabe der Geschichte
        5. Download-Button f√ºr die fertige Story
        """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### üí° Live-Demo")
            
            selected_model = add_select_model(key="practice_ex4_model")
            
            col_a, col_b = st.columns(2)
            with col_a:
                genre = st.selectbox(
                    "Genre:",
                    ["Fantasy", "Sci-Fi", "Mystery", "Romance", "Horror", "Abenteuer"],
                    key="practice_ex4_genre"
                )
            with col_b:
                length = st.select_slider(
                    "Story-L√§nge:",
                    ["Kurz", "Mittel", "Lang"],
                    value="Mittel",
                    key="practice_ex4_length"
                )
            
            character = st.text_input(
                "Hauptcharakter:",
                "Ein mutiger Astronaut",
                key="practice_ex4_char"
            )
            
            setting = st.text_input(
                "Setting/Ort:",
                "Auf einem fernen Planeten",
                key="practice_ex4_setting"
            )
            
            temperature = st.slider(
                "Kreativit√§t (Temperature):",
                0.0, 2.0, 0.9, 0.1,
                key="practice_ex4_temp"
            )
            
            if st.button("üìö Story generieren", key="practice_ex4_button", type="primary"):
                length_map = {
                    "Kurz": "eine kurze Geschichte (ca. 100-150 W√∂rter)",
                    "Mittel": "eine mittellange Geschichte (ca. 200-300 W√∂rter)",
                    "Lang": "eine l√§ngere Geschichte (ca. 400-500 W√∂rter)"
                }
                
                prompt = f"""Schreibe {length_map[length]} im {genre}-Genre.

Hauptcharakter: {character}
Setting: {setting}

Die Geschichte sollte spannend sein und eine klare Struktur haben (Anfang, Konflikt, L√∂sung)."""
                
                with st.spinner("Story wird generiert..."):
                    try:
                        import ollama
                        
                        st.markdown("### üìñ Deine Story:")
                        st.markdown("---")
                        
                        placeholder = st.empty()
                        full_story = ""
                        
                        stream = ollama.generate(
                            model=selected_model,
                            prompt=prompt,
                            options={'temperature': temperature},
                            stream=True
                        )
                        
                        for chunk in stream:
                            if 'response' in chunk:
                                full_story += chunk['response']
                                placeholder.markdown(full_story + "‚ñå")
                        
                        placeholder.markdown(full_story)
                        
                        st.markdown("---")
                        st.success("‚úÖ Story abgeschlossen!")
                        
                        # Download
                        st.download_button(
                            "üì• Story herunterladen",
                            full_story,
                            f"story_{genre.lower()}.txt",
                            "text/plain"
                        )
                        
                        # Stats
                        word_count = len(full_story.split())
                        st.info(f"üìä Wortanzahl: {word_count} W√∂rter")
                    except Exception as e:
                        st.error(f"Fehler: {str(e)}")
        
        with col2:
            st.markdown("### üíª L√∂sung")
            st.code('''import streamlit as st
import ollama
from lib.helper_streamlit import add_select_model

st.title("Story-Generator")

model = add_select_model()

genre = st.selectbox(
    "Genre:",
    ["Fantasy", "Sci-Fi", "Mystery", "Romance"]
)

length = st.select_slider(
    "L√§nge:",
    ["Kurz", "Mittel", "Lang"]
)

character = st.text_input("Hauptcharakter:", "Ein mutiger Held")
setting = st.text_input("Setting:", "In einem magischen Wald")

temperature = st.slider("Kreativit√§t:", 0.0, 2.0, 0.9)

if st.button("Story generieren"):
    length_map = {
        "Kurz": "eine kurze Geschichte (100-150 W√∂rter)",
        "Mittel": "eine mittellange Geschichte (200-300 W√∂rter)",
        "Lang": "eine l√§ngere Geschichte (400-500 W√∂rter)"
    }
    
    prompt = f"""Schreibe {length_map[length]} im {genre}-Genre.
    
    Hauptcharakter: {character}
    Setting: {setting}
    
    Mit klarer Struktur (Anfang, Konflikt, L√∂sung)."""
    
    placeholder = st.empty()
    full_story = ""
    
    stream = ollama.generate(
        model=model,
        prompt=prompt,
        options={'temperature': temperature},
        stream=True
    )
    
    for chunk in stream:
        if 'response' in chunk:
            full_story += chunk['response']
            placeholder.markdown(full_story + "‚ñå")
    
    placeholder.markdown(full_story)
    
    # Download
    st.download_button(
        "Herunterladen",
        full_story,
        f"story_{genre}.txt"
    )
''', language='python')
    
    # Tipps und Zusammenfassung
    st.divider()
    st.markdown("""
    ### üéØ √úbungsziele erreicht?
    
    Nach diesen √úbungen sollten Sie:
    - ‚úÖ Ollama f√ºr verschiedene Aufgaben einsetzen k√∂nnen
    - ‚úÖ Prompts effektiv gestalten k√∂nnen
    - ‚úÖ Streaming-Responses implementieren k√∂nnen
    - ‚úÖ Parameter wie Temperature richtig nutzen k√∂nnen
    - ‚úÖ Ollama mit Streamlit integrieren k√∂nnen
    
    ### üí° Tipps f√ºr eigene Projekte:
    1. **Klare Prompts**: Je spezifischer, desto besser die Ergebnisse
    2. **Temperature**: 0.0-0.3 f√ºr faktische, 0.7-1.5 f√ºr kreative Aufgaben
    3. **Streaming**: Verbessert UX bei l√§ngeren Antworten
    4. **Error Handling**: Immer try-except f√ºr API-Calls verwenden
    5. **Model Selection**: Gr√∂√üere Modelle ‚â† immer besser f√ºr alle Aufgaben
    """)

# Zusammenfassung
st.divider()
st.header("üìö Zusammenfassung Tag 3")

col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("""
    ### ‚úÖ Gelernt
    - Ollama installieren
    - Modelle verwalten
    - Python API nutzen
    - Chat erstellen
    - Streaming implementieren
    """)

with col2:
    st.markdown("""
    ### üéØ Key Methods
    - `ollama.generate()` - Text generieren
    - `ollama.chat()` - Chat mit History
    - `stream=True` - Streaming
    - `options={}` - Parameter
    - System Prompts
    """)

with col3:
    st.markdown("""
    ### üöÄ Tag 4
    - Vollst√§ndige Apps
    - Chat-Interfaces
    - Text-Generator
    - Code-Assistent
    - Best Practices
    """)

# =================================================================================================
show_code(__file__)
