"""
Tag 3: Streamlit Ollama Apps

Entwicklung vollst√§ndiger KI-Anwendungen
"""

import streamlit as st
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from lib.helper_streamlit import show_code, add_select_model
from lib.helper_ollama import get_available_models
import ollama

st.set_page_config(
    page_title="Tag 3: Streamlit Ollama Apps",
    page_icon="üí°",
    layout="wide"
)

st.title("üí° Tag 3: Streamlit Ollama Apps")
st.markdown("**Entwicklung vollst√§ndiger KI-Anwendungen**")

# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è Konfiguration")
    selected_model = add_select_model()
    
    st.divider()
    
    temperature = st.slider("Temperature:", 0.0, 2.0, 0.7, 0.1)
    max_tokens = st.number_input("Max Tokens:", 50, 2000, 500, 50)

# Tabs
TAB_OVERVIEW = "√úbersicht"
TAB_CHAT_APP = "Chat-App"
TAB_TEXT_GENERATOR = "Text-Generator"
TAB_CODE_ASSISTANT = "Code-Assistent"
TAB_SUMMARY_TOOL = "Zusammenfassung-Tool"
TAB_MULTI_MODEL = "Multi-Model"
TAB_BEST_PRACTICES = "Best Practices"

TAB_NAMES = [
    TAB_OVERVIEW,
    TAB_CHAT_APP,
    TAB_TEXT_GENERATOR,
    TAB_CODE_ASSISTANT,
    TAB_SUMMARY_TOOL,
    TAB_MULTI_MODEL,
    TAB_BEST_PRACTICES,
]

tabs = st.tabs(TAB_NAMES)

def get_tab_index(name):
    try:
        return TAB_NAMES.index(name)
    except ValueError:
        return -1

# Tab 1: √úbersicht
with tabs[get_tab_index(TAB_OVERVIEW)]:
    st.header("üìã Kurs√ºbersicht Tag 3")
    
    st.markdown("""
    ### Lernziele
    Am Ende von Tag 3 k√∂nnen Sie:
    - ‚úÖ Vollst√§ndige Chat-Anwendungen erstellen
    - ‚úÖ Streaming-Responses implementieren
    - ‚úÖ Verschiedene App-Typen entwickeln
    - ‚úÖ Multi-Model-Vergleiche durchf√ºhren
    - ‚úÖ Best Practices anwenden
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### üìö Projekte
        1. **Chat-App** - Interaktiver Assistent
        2. **Text-Generator** - Kreative Inhalte
        3. **Code-Assistent** - Programmierhilfe
        4. **Zusammenfassung** - Text-Analyse
        5. **Multi-Model** - Modellvergleich
        """)
    
    with col2:
        st.markdown("""
        ### ‚è±Ô∏è Zeitplan
        - **09:00 - 10:30**: Chat-App
        - **10:30 - 10:45**: Pause
        - **10:45 - 12:00**: Generator & Code
        - **12:00 - 13:00**: Mittagspause
        - **13:00 - 14:30**: Tools & Vergleich
        - **14:30 - 15:00**: Best Practices
        """)

# Tab 2: Chat-App
with tabs[get_tab_index(TAB_CHAT_APP)]:
    st.header("üí¨ Chat-App mit History")
    
    st.markdown("""
    ### Vollst√§ndige Chat-Anwendung
    Eine professionelle Chat-App mit Nachrichtenverlauf, System-Prompts und Streaming.
    """)
    
    # System Prompt
    system_prompt = st.text_area(
        "System Prompt:",
        "Du bist ein hilfreicher KI-Assistent, der pr√§zise und freundliche Antworten gibt.",
        height=100,
        key="chat_system"
    )
    
    # Chat History initialisieren
    if 'chat_messages' not in st.session_state:
        st.session_state.chat_messages = []
    
    # Nachrichten anzeigen
    st.markdown("### üí¨ Konversation")
    
    for msg in st.session_state.chat_messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
    
    # Chat Input
    if prompt := st.chat_input("Ihre Nachricht..."):
        # User message hinzuf√ºgen und anzeigen
        st.session_state.chat_messages.append({
            "role": "user",
            "content": prompt
        })
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Assistant response mit Streaming
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            try:
                # Messages f√ºr API vorbereiten
                messages = [{"role": "system", "content": system_prompt}]
                messages.extend(st.session_state.chat_messages)
                
                # Streaming Response
                stream = ollama.chat(
                    model=selected_model,
                    messages=messages,
                    stream=True,
                    options={
                        'temperature': temperature,
                        'num_predict': max_tokens
                    }
                )
                
                for chunk in stream:
                    if 'message' in chunk and 'content' in chunk['message']:
                        full_response += chunk['message']['content']
                        message_placeholder.markdown(full_response + "‚ñå")
                
                message_placeholder.markdown(full_response)
                
                # Response zur History hinzuf√ºgen
                st.session_state.chat_messages.append({
                    "role": "assistant",
                    "content": full_response
                })
                
            except Exception as e:
                st.error(f"‚ùå Fehler: {e}")
    
    # Controls
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üóëÔ∏è Chat l√∂schen", key="clear_chat"):
            st.session_state.chat_messages = []
            st.rerun()
    
    with col2:
        if st.button("üíæ Exportieren", key="export_chat"):
            if st.session_state.chat_messages:
                import json
                chat_export = json.dumps(st.session_state.chat_messages, indent=2, ensure_ascii=False)
                st.download_button(
                    "üì• Download JSON",
                    chat_export,
                    "chat_export.json",
                    "application/json"
                )
    
    with col3:
        msg_count = len(st.session_state.chat_messages)
        st.metric("Nachrichten", msg_count)

# Tab 3: Text-Generator
with tabs[get_tab_index(TAB_TEXT_GENERATOR)]:
    st.header("‚úçÔ∏è Text-Generator")
    
    st.markdown("### Kreative Texte generieren")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("‚öôÔ∏è Einstellungen")
        
        text_type = st.selectbox(
            "Text-Art:",
            ["Blog-Post", "Geschichte", "Gedicht", "Email", "Produktbeschreibung", "Social Media"],
            key="text_type"
        )
        
        text_topic = st.text_input(
            "Thema:",
            "K√ºnstliche Intelligenz",
            key="text_topic"
        )
        
        text_length = st.select_slider(
            "L√§nge:",
            options=["Kurz", "Mittel", "Lang"],
            value="Mittel",
            key="text_length"
        )
        
        text_tone = st.selectbox(
            "Ton:",
            ["Professionell", "Freundlich", "Formal", "Humorvoll", "Informativ"],
            key="text_tone"
        )
        
        additional_instructions = st.text_area(
            "Zus√§tzliche Anweisungen:",
            placeholder="z.B. 'Verwende einfache Sprache' oder 'F√ºge Beispiele hinzu'",
            height=100,
            key="additional_instr"
        )
        
        if st.button("‚ú® Generieren", type="primary", key="generate_text"):
            # Prompt zusammenstellen
            length_map = {"Kurz": "kurzen", "Mittel": "mittellangen", "Lang": "langen"}
            
            prompt = f"""Schreibe einen {length_map[text_length]} {text_type} zum Thema: {text_topic}.

Ton: {text_tone}

{f'Zus√§tzliche Anweisungen: {additional_instructions}' if additional_instructions else ''}

Bitte generiere qualitativ hochwertigen Inhalt."""
            
            with col2:
                st.subheader("üìÑ Generierter Text")
                
                with st.spinner("Generiere..."):
                    try:
                        response_placeholder = st.empty()
                        full_text = ""
                        
                        stream = ollama.generate(
                            model=selected_model,
                            prompt=prompt,
                            stream=True,
                            options={
                                'temperature': temperature,
                                'num_predict': max_tokens
                            }
                        )
                        
                        for chunk in stream:
                            if 'response' in chunk:
                                full_text += chunk['response']
                                response_placeholder.markdown(full_text + "‚ñå")
                        
                        response_placeholder.markdown(full_text)
                        
                        # Download Button
                        st.download_button(
                            "üì• Text herunterladen",
                            full_text,
                            f"{text_type}_{text_topic}.txt",
                            "text/plain"
                        )
                        
                    except Exception as e:
                        st.error(f"‚ùå Fehler: {e}")
    
    with col2:
        if 'generate_text' not in st.session_state or not st.session_state.get('generate_text'):
            st.subheader("üìÑ Generierter Text")
            st.info("üëà Konfigurieren Sie die Einstellungen und klicken Sie auf 'Generieren'")

# Tab 4: Code-Assistent
with tabs[get_tab_index(TAB_CODE_ASSISTANT)]:
    st.header("üíª Code-Assistent")
    
    st.markdown("### Programmierhilfe mit KI")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üéØ Aufgabe")
        
        code_task = st.selectbox(
            "Was m√∂chten Sie tun?",
            [
                "Code schreiben",
                "Code erkl√§ren",
                "Code debuggen",
                "Code optimieren",
                "Tests schreiben"
            ],
            key="code_task"
        )
        
        programming_language = st.selectbox(
            "Programmiersprache:",
            ["Python", "JavaScript", "Java", "C++", "Go", "Rust"],
            key="prog_lang"
        )
        
        if code_task == "Code schreiben":
            code_description = st.text_area(
                "Beschreiben Sie, was der Code tun soll:",
                "Eine Funktion, die eine Liste von Zahlen sortiert",
                height=100,
                key="code_desc"
            )
            user_code = None
        else:
            code_description = st.text_area(
                "Zus√§tzliche Informationen:",
                height=50,
                key="code_info"
            )
            user_code = st.text_area(
                "Ihr Code:",
                height=200,
                key="user_code"
            )
        
        if st.button("üöÄ Ausf√ºhren", type="primary", key="run_code_assistant"):
            with col2:
                st.subheader("üí° Ergebnis")
                
                # Prompt je nach Task
                task_prompts = {
                    "Code schreiben": f"Schreibe {programming_language} Code f√ºr: {code_description}. F√ºge Kommentare hinzu.",
                    "Code erkl√§ren": f"Erkl√§re diesen {programming_language} Code:\n\n{user_code}",
                    "Code debuggen": f"Finde und behebe Fehler in diesem {programming_language} Code:\n\n{user_code}\n\nZus√§tzliche Info: {code_description}",
                    "Code optimieren": f"Optimiere diesen {programming_language} Code:\n\n{user_code}\n\nZiel: {code_description}",
                    "Tests schreiben": f"Schreibe Unit-Tests f√ºr diesen {programming_language} Code:\n\n{user_code}"
                }
                
                prompt = task_prompts[code_task]
                
                with st.spinner("Verarbeite..."):
                    try:
                        response_placeholder = st.empty()
                        full_response = ""
                        
                        stream = ollama.generate(
                            model=selected_model,
                            prompt=prompt,
                            stream=True,
                            options={'temperature': 0.3}  # Niedrigere Temp f√ºr Code
                        )
                        
                        for chunk in stream:
                            if 'response' in chunk:
                                full_response += chunk['response']
                                response_placeholder.markdown(full_response + "‚ñå")
                        
                        response_placeholder.markdown(full_response)
                        
                    except Exception as e:
                        st.error(f"‚ùå Fehler: {e}")
    
    with col2:
        if 'run_code_assistant' not in st.session_state or not st.session_state.get('run_code_assistant'):
            st.subheader("üí° Ergebnis")
            st.info("üëà W√§hlen Sie eine Aufgabe und klicken Sie auf 'Ausf√ºhren'")

# Tab 5: Zusammenfassung-Tool
with tabs[get_tab_index(TAB_SUMMARY_TOOL)]:
    st.header("üìù Zusammenfassung-Tool")
    
    st.markdown("### Texte automatisch zusammenfassen")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üìÑ Eingabe")
        
        input_text = st.text_area(
            "Text zum Zusammenfassen:",
            placeholder="F√ºgen Sie hier den Text ein, den Sie zusammenfassen m√∂chten...",
            height=300,
            key="summary_input"
        )
        
        summary_length = st.select_slider(
            "Zusammenfassungsl√§nge:",
            options=["Sehr kurz (1-2 S√§tze)", "Kurz (3-5 S√§tze)", "Mittel (1 Absatz)", "Detailliert (mehrere Abs√§tze)"],
            value="Kurz (3-5 S√§tze)",
            key="summary_length"
        )
        
        summary_style = st.selectbox(
            "Stil:",
            ["Bullet Points", "Flie√ütext", "Strukturiert"],
            key="summary_style"
        )
        
        col_a, col_b = st.columns(2)
        
        with col_a:
            if st.button("üìä Zusammenfassen", type="primary", key="summarize"):
                if input_text:
                    with col2:
                        st.subheader("üìã Zusammenfassung")
                        
                        prompt = f"""Fasse den folgenden Text zusammen.

L√§nge: {summary_length}
Stil: {summary_style}

Text:
{input_text}

Zusammenfassung:"""
                        
                        with st.spinner("Fasse zusammen..."):
                            try:
                                response_placeholder = st.empty()
                                full_summary = ""
                                
                                stream = ollama.generate(
                                    model=selected_model,
                                    prompt=prompt,
                                    stream=True,
                                    options={'temperature': 0.3}
                                )
                                
                                for chunk in stream:
                                    if 'response' in chunk:
                                        full_summary += chunk['response']
                                        response_placeholder.markdown(full_summary + "‚ñå")
                                
                                response_placeholder.markdown(full_summary)
                                
                                # Statistiken
                                st.divider()
                                col_i, col_ii, col_iii = st.columns(3)
                                col_i.metric("Original W√∂rter", len(input_text.split()))
                                col_ii.metric("Zusammenfassung W√∂rter", len(full_summary.split()))
                                reduction = (1 - len(full_summary.split()) / len(input_text.split())) * 100
                                col_iii.metric("Reduktion", f"{reduction:.1f}%")
                                
                            except Exception as e:
                                st.error(f"‚ùå Fehler: {e}")
                else:
                    st.warning("Bitte geben Sie einen Text ein.")
        
        with col_b:
            if st.button("üîÑ Beispieltext laden", key="load_example"):
                st.session_state.summary_input = """Python ist eine interpretierte, objektorientierte Programmiersprache mit dynamischer Semantik. 
Ihre high-level eingebauten Datenstrukturen, kombiniert mit dynamischer Typisierung und dynamischem Binding, machen sie sehr attraktiv f√ºr Rapid Application Development, 
sowie f√ºr die Verwendung als Scripting- oder Glue-Language, um bestehende Komponenten miteinander zu verbinden. Pythons einfache, leicht zu erlernende Syntax betont 
Lesbarkeit und reduziert daher die Kosten f√ºr Programm-Wartung. Python unterst√ºtzt Module und Pakete, was Modularit√§t und Code-Wiederverwendung f√∂rdert."""
                st.rerun()
    
    with col2:
        if 'summarize' not in st.session_state or not st.session_state.get('summarize'):
            st.subheader("üìã Zusammenfassung")
            st.info("üëà Geben Sie einen Text ein und klicken Sie auf 'Zusammenfassen'")

# Tab 6: Multi-Model Vergleich
with tabs[get_tab_index(TAB_MULTI_MODEL)]:
    st.header("üî¨ Multi-Model Vergleich")
    
    st.markdown("### Verschiedene Modelle vergleichen")
    
    available_models = get_available_models()
    
    if len(available_models) >= 2:
        col1, col2 = st.columns([1, 2])
        
        with col1:
            st.subheader("‚öôÔ∏è Konfiguration")
            
            model1 = st.selectbox("Modell 1:", available_models, key="model1")
            model2 = st.selectbox("Modell 2:", available_models, key="model2", index=1 if len(available_models) > 1 else 0)
            
            compare_prompt = st.text_area(
                "Prompt:",
                "Erkl√§re Machine Learning in einfachen Worten.",
                height=100,
                key="compare_prompt"
            )
            
            if st.button("‚ö° Vergleichen", type="primary", key="compare_models"):
                with col2:
                    st.subheader("üìä Ergebnisse")
                    
                    col_a, col_b = st.columns(2)
                    
                    # Modell 1
                    with col_a:
                        st.markdown(f"**{model1}**")
                        
                        with st.spinner(f"Generiere mit {model1}..."):
                            try:
                                import time
                                start_time = time.time()
                                
                                response1 = ollama.generate(
                                    model=model1,
                                    prompt=compare_prompt,
                                    options={'temperature': temperature}
                                )
                                
                                duration1 = time.time() - start_time
                                
                                st.markdown(response1['response'])
                                st.caption(f"‚è±Ô∏è {duration1:.2f}s")
                                
                            except Exception as e:
                                st.error(f"Fehler: {e}")
                    
                    # Modell 2
                    with col_b:
                        st.markdown(f"**{model2}**")
                        
                        with st.spinner(f"Generiere mit {model2}..."):
                            try:
                                import time
                                start_time = time.time()
                                
                                response2 = ollama.generate(
                                    model=model2,
                                    prompt=compare_prompt,
                                    options={'temperature': temperature}
                                )
                                
                                duration2 = time.time() - start_time
                                
                                st.markdown(response2['response'])
                                st.caption(f"‚è±Ô∏è {duration2:.2f}s")
                                
                            except Exception as e:
                                st.error(f"Fehler: {e}")
        
        with col2:
            if 'compare_models' not in st.session_state or not st.session_state.get('compare_models'):
                st.subheader("üìä Ergebnisse")
                st.info("üëà W√§hlen Sie zwei Modelle und klicken Sie auf 'Vergleichen'")
    
    else:
        st.warning("‚ö†Ô∏è Mindestens 2 Modelle erforderlich f√ºr Vergleich")
        st.info("Installieren Sie weitere Modelle mit `ollama pull <model-name>`")

# Tab 7: Best Practices
with tabs[get_tab_index(TAB_BEST_PRACTICES)]:
    st.header("‚ú® Best Practices")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### üéØ UI/UX Best Practices
        
        **1. Streaming verwenden**
        ```python
        # Besser: Mit Streaming
        stream = ollama.generate(..., stream=True)
        for chunk in stream:
            # Update UI progressiv
        ```
        
        **2. Ladeanzeigen**
        ```python
        with st.spinner("Generiere..."):
            response = ollama.generate(...)
        ```
        
        **3. Fehlerbehandlung**
        ```python
        try:
            response = ollama.generate(...)
        except Exception as e:
            st.error(f"Fehler: {e}")
            # Fallback-Option anbieten
        ```
        
        **4. Session State**
        ```python
        if 'history' not in st.session_state:
            st.session_state.history = []
        ```
        
        **5. Feedback geben**
        ```python
        st.success("‚úÖ Erfolgreich!")
        st.info("‚ÑπÔ∏è Hinweis: ...")
        st.warning("‚ö†Ô∏è Achtung!")
        ```
        """)
    
    with col2:
        st.markdown("""
        ### ‚öôÔ∏è Performance Best Practices
        
        **1. Optimale Parameter**
        - Temperature: 0.1-0.3 f√ºr faktische Antworten
        - Temperature: 0.7-0.9 f√ºr kreative Texte
        - Max Tokens begrenzen
        
        **2. Modell-Auswahl**
        - Kleine Modelle (2-4GB) f√ºr einfache Tasks
        - Gro√üe Modelle (7GB+) f√ºr komplexe Aufgaben
        
        **3. Caching**
        ```python
        @st.cache_data
        def get_model_list():
            return ollama.list()
        ```
        
        **4. Kontext-Management**
        - Konversations-History begrenzen
        - Alte Nachrichten zusammenfassen
        
        **5. Prompt Engineering**
        - Klare, spezifische Anweisungen
        - Beispiele im Prompt verwenden
        - System Prompts f√ºr Konsistenz
        """)
    
    st.divider()
    
    st.markdown("### üìã Checkliste f√ºr Production-Apps")
    
    checklist = {
        "‚úÖ Fehlerbehandlung implementiert": False,
        "‚úÖ Streaming f√ºr bessere UX": False,
        "‚úÖ Session State f√ºr Zustandsverwaltung": False,
        "‚úÖ Benutzer-Feedback bei langen Operationen": False,
        "‚úÖ Input-Validierung": False,
        "‚úÖ Modell-Verf√ºgbarkeit pr√ºfen": False,
        "‚úÖ Parameter konfigurierbar": False,
        "‚úÖ Export-Funktionen": False,
        "‚úÖ Responsive Design": False,
        "‚úÖ Dokumentation": False
    }
    
    for item in checklist:
        st.checkbox(item, key=f"checklist_{item}")

# Zusammenfassung
st.divider()
st.header("üìö Zusammenfassung Tag 3")

col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("""
    ### ‚úÖ Gelernt
    - Chat-Apps mit Streaming
    - Text-Generator
    - Code-Assistent
    - Zusammenfassungs-Tool
    - Multi-Model-Vergleich
    """)

with col2:
    st.markdown("""
    ### üéØ Projekte
    - 5 vollst√§ndige Apps
    - Best Practices
    - Production-ready Code
    - Performance-Optimierung
    """)

with col3:
    st.markdown("""
    ### üöÄ Tag 4
    - Komplette Anwendung
    - Deployment
    - Erweiterte Features
    - Abschlussprojekt
    """)

# =================================================================================================
show_code(__file__)
